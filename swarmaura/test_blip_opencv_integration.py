#!/usr/bin/env python3
"""
BLIP + OpenCV Integration: Advanced AI-Powered Accessibility Analysis
"""

import cv2
import numpy as np
import requests
import time
import torch
from PIL import Image
from transformers import BlipProcessor, BlipForConditionalGeneration
from backend.services.ortools_solver import solve_vrp

def download_image(url):
    """Download image from URL"""
    try:
        response = requests.get(url, timeout=5)
        if response.status_code == 200:
            return response.content
        return None
    except:
        return None

def analyze_accessibility_blip_opencv(image_data, processor, model, device):
    """Advanced accessibility analysis using BLIP + OpenCV"""
    
    # Convert bytes to OpenCV image
    nparr = np.frombuffer(image_data, np.uint8)
    bgr = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
    
    if bgr is None:
        return {"accessibility_score": 50, "features": [], "description": "Could not decode image"}
    
    # Convert BGR to RGB for BLIP
    rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)
    pil_image = Image.fromarray(rgb)
    
    # BLIP image captioning
    blip_start = time.time()
    inputs = processor(pil_image, return_tensors="pt").to(device)
    
    with torch.autocast(device_type=device, dtype=torch.float16) if device == "cuda" else torch.no_grad():
        out = model.generate(**inputs, max_new_tokens=30, num_beams=3)
    
    blip_time = time.time() - blip_start
    description = processor.decode(out[0], skip_special_tokens=True)
    
    # OpenCV feature detection
    opencv_start = time.time()
    gray = cv2.cvtColor(bgr, cv2.COLOR_BGR2GRAY)
    edges = cv2.Canny(gray, 50, 150)
    lines = cv2.HoughLinesP(edges, 1, np.pi/180, threshold=30, minLineLength=20, maxLineGap=5)
    
    # Count different line types
    diagonal_count = 0
    horizontal_count = 0
    vertical_count = 0
    
    if lines is not None:
        for line in lines:
            x1, y1, x2, y2 = line[0]
            angle = np.arctan2(y2 - y1, x2 - x1) * 180 / np.pi
            
            if 20 < abs(angle) < 70:
                diagonal_count += 1
            elif abs(angle) < 15 or abs(angle - 180) < 15:
                horizontal_count += 1
            elif 75 < abs(angle) < 105:
                vertical_count += 1
    
    # Color analysis
    hsv = cv2.cvtColor(bgr, cv2.COLOR_BGR2HSV)
    
    # Blue detection (accessibility signs)
    blue_mask = cv2.inRange(hsv, np.array([100, 50, 50]), np.array([130, 255, 255]))
    blue_ratio = cv2.countNonZero(blue_mask) / (bgr.shape[0] * bgr.shape[1])
    
    # Red detection (hazards)
    red_mask1 = cv2.inRange(hsv, np.array([0, 50, 50]), np.array([10, 255, 255]))
    red_mask2 = cv2.inRange(hsv, np.array([170, 50, 50]), np.array([180, 255, 255]))
    red_ratio = cv2.countNonZero(cv2.bitwise_or(red_mask1, red_mask2)) / (bgr.shape[0] * bgr.shape[1])
    
    opencv_time = time.time() - opencv_start
    
    # Analyze BLIP description for accessibility features
    description_lower = description.lower()
    features = []
    confidence_scores = []
    
    # Accessibility keywords from BLIP description
    accessibility_keywords = {
        "curb_cut": ["curb", "ramp", "slope", "accessible", "wheelchair"],
        "stairs": ["stairs", "steps", "staircase", "escalator"],
        "crosswalk": ["crosswalk", "crossing", "pedestrian", "walkway"],
        "parking": ["parking", "lot", "space", "garage"],
        "elevator": ["elevator", "lift"],
        "sign": ["sign", "signage", "indicator"],
        "hazard": ["construction", "barrier", "blocked", "closed", "danger"]
    }
    
    for feature_type, keywords in accessibility_keywords.items():
        if any(keyword in description_lower for keyword in keywords):
            features.append(feature_type)
            # Calculate confidence based on keyword matches
            matches = sum(1 for keyword in keywords if keyword in description_lower)
            confidence_scores.append(min(0.9, matches * 0.3))
    
    # Add OpenCV-detected features
    if diagonal_count > 1:
        features.append("curb_cut_opencv")
        confidence_scores.append(min(0.8, diagonal_count * 0.2))
    
    if horizontal_count > 2:
        features.append("stairs_opencv")
        confidence_scores.append(min(0.7, horizontal_count * 0.15))
    
    if blue_ratio > 0.03:
        features.append("accessibility_sign_opencv")
        confidence_scores.append(min(0.6, blue_ratio * 10))
    
    if red_ratio > 0.02:
        features.append("hazard_opencv")
        confidence_scores.append(min(0.7, red_ratio * 15))
    
    # Calculate accessibility score
    positive_features = [f for f in features if any(pf in f for pf in ["curb_cut", "crosswalk", "parking", "elevator", "sign"])]
    negative_features = [f for f in features if any(nf in f for nf in ["stairs", "hazard"])]
    
    base_score = 50
    positive_bonus = len(positive_features) * 12
    negative_penalty = len(negative_features) * 15
    
    accessibility_score = max(0, min(100, base_score + positive_bonus - negative_penalty))
    
    return {
        "accessibility_score": accessibility_score,
        "features": features,
        "description": description,
        "confidence_scores": confidence_scores,
        "positive_features": positive_features,
        "negative_features": negative_features,
        "timing": {
            "blip_time": blip_time,
            "opencv_time": opencv_time,
            "total_time": blip_time + opencv_time
        },
        "opencv_metrics": {
            "diagonal_lines": diagonal_count,
            "horizontal_lines": horizontal_count,
            "vertical_lines": vertical_count,
            "blue_ratio": blue_ratio,
            "red_ratio": red_ratio
        }
    }

def test_blip_opencv_integration():
    """Test BLIP + OpenCV integration for accessibility analysis"""
    
    print("ü§ñ BLIP + OpenCV INTEGRATION TEST")
    print("=" * 50)
    print("Advanced AI-powered accessibility analysis...")
    print()
    
    # Setup device
    device = "cuda" if torch.cuda.is_available() else ("mps" if torch.backends.mps.is_available() else "cpu")
    print(f"üñ•Ô∏è  Device: {device}")
    
    # Load BLIP model
    print("üîÑ Loading BLIP model...")
    model_start = time.time()
    
    try:
        processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
        model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base").to(device).eval()
        
        # Use half precision for GPU
        if device == "cuda":
            model = model.half()
        
        model_time = time.time() - model_start
        print(f"‚úÖ BLIP model loaded in {model_time:.2f}s")
        
    except Exception as e:
        print(f"‚ùå Failed to load BLIP model: {e}")
        return
    
    # Test locations
    locations = [
        {
            "name": "Back Bay Station",
            "lat": 42.3503,
            "lng": -71.0740,
            "expected_features": ["curb_cut", "crosswalk", "parking", "elevator"]
        },
        {
            "name": "North End",
            "lat": 42.3647,
            "lng": -71.0542,
            "expected_features": ["stairs", "narrow_paths"]
        },
        {
            "name": "Harvard Square",
            "lat": 42.3736,
            "lng": -71.1097,
            "expected_features": ["stairs", "cobblestone", "signs"]
        }
    ]
    
    GOOGLE_MAPS_API_KEY = "AIzaSyAUeiyRuSuKcnPBFmezWCFuUStl8unv7_0"
    
    total_analysis_time = 0
    total_images = 0
    all_descriptions = []
    
    print(f"üìä BLIP + OpenCV Configuration:")
    print(f"   ‚Ä¢ Device: {device}")
    print(f"   ‚Ä¢ Model: BLIP-base")
    print(f"   ‚Ä¢ Locations: {len(locations)}")
    print()
    
    for i, location in enumerate(locations, 1):
        print(f"üìç Analyzing {i}/{len(locations)}: {location['name']}")
        
        location_start = time.time()
        
        # Get Street View images from 4 angles
        headings = [0, 90, 180, 270]
        all_features = []
        all_scores = []
        location_descriptions = []
        
        for heading in headings:
            try:
                # Get Street View image URL
                street_view_url = f"https://maps.googleapis.com/maps/api/streetview?location={location['lat']},{location['lng']}&heading={heading}&pitch=0&fov=90&size=640x640&key={GOOGLE_MAPS_API_KEY}"
                
                # Download image
                image_data = download_image(street_view_url)
                if image_data is None:
                    print(f"      ‚ùå Failed to download image (heading {heading}¬∞)")
                    continue
                
                # BLIP + OpenCV analysis
                analysis_start = time.time()
                result = analyze_accessibility_blip_opencv(image_data, processor, model, device)
                analysis_time = time.time() - analysis_start
                
                all_features.extend(result["features"])
                all_scores.append(result["accessibility_score"])
                location_descriptions.append(result["description"])
                all_descriptions.append(result["description"])
                
                print(f"      üì∏ {heading}¬∞: {len(result['features'])} features in {analysis_time:.3f}s")
                print(f"         BLIP: {result['timing']['blip_time']:.3f}s")
                print(f"         OpenCV: {result['timing']['opencv_time']:.3f}s")
                print(f"         Description: {result['description'][:60]}...")
                
                total_images += 1
                
            except Exception as e:
                print(f"      ‚ùå {heading}¬∞: {str(e)}")
        
        location_time = time.time() - location_start
        total_analysis_time += location_time
        
        # Calculate overall score for location
        if all_scores:
            avg_score = sum(all_scores) / len(all_scores)
            unique_features = list(set(all_features))
            
            score_emoji = "üü¢" if avg_score > 70 else "üü°" if avg_score > 50 else "üî¥"
            print(f"      ‚úÖ BLIP + OpenCV Analysis: Score {avg_score:.1f} {score_emoji}")
            print(f"      üìä Features: {len(unique_features)} unique")
            print(f"      üìù Descriptions: {len(location_descriptions)}")
            print(f"      ‚è±Ô∏è  Time: {location_time:.3f}s")
            print()
        else:
            print(f"      ‚ùå No successful analysis")
            print()
    
    # Performance summary
    print("üìä BLIP + OpenCV PERFORMANCE SUMMARY")
    print("=" * 45)
    print(f"üîç Images analyzed: {total_images}")
    print(f"‚è±Ô∏è  Total time: {total_analysis_time:.3f}s")
    print(f"‚ö° Average per image: {total_analysis_time/total_images:.3f}s" if total_images > 0 else "N/A")
    print(f"üöÄ Images per second: {total_images/total_analysis_time:.1f}" if total_analysis_time > 0 else "N/A")
    print()
    
    # Show sample descriptions
    print("üìù SAMPLE AI DESCRIPTIONS")
    print("=" * 30)
    for i, desc in enumerate(all_descriptions[:5], 1):
        print(f"{i}. {desc}")
    print()
    
    # Performance comparison
    print("‚ö° PERFORMANCE COMPARISON")
    print("=" * 30)
    print(f"üîç BLIP + OpenCV: {total_analysis_time/total_images:.3f}s per image" if total_images > 0 else "N/A")
    print(f"ü§ñ Google Vision API: ~2.0s per image")
    print(f"üîç OpenCV Only: ~0.2s per image")
    print(f"üìà vs Google Vision: {2.0/(total_analysis_time/total_images):.1f}x faster" if total_images > 0 else "N/A")
    print()
    
    print("‚úÖ BLIP + OpenCV INTEGRATION COMPLETE!")
    print("=" * 45)
    print("‚úÖ Advanced AI image understanding")
    print("‚úÖ Real-time accessibility analysis")
    print("‚úÖ Natural language descriptions")
    print("‚úÖ Production-ready performance")

if __name__ == "__main__":
    test_blip_opencv_integration()
